# Machine-Learning-Based-Speech-Emotion-Recognition
This project utilizes two types of neural networks for speech emotion recognition: Long Short-Term Memory (LSTM) and Convolutional Neural Network (CNN) models.

Long Short-Term Memory (LSTM) Model
LSTM is a type of recurrent neural network (RNN) designed to capture long-term dependencies in sequential data. It is particularly effective for tasks involving time series or sequential data, such as speech. In this project, the LSTM model is trained on a dataset of speech samples labeled with various emotions, including happiness, sadness, and anger.

Convolutional Neural Network (CNN) Model
CNNs are commonly used for image processing but can also be applied to speech emotion recognition by transforming audio signals into spectrograms. Spectrograms are visual representations of the frequency spectrum of a signal as it varies with time. The CNN model captures spatial hierarchies and patterns in these spectrograms to classify different emotions in speech.

Dataset
The Toronto Emotional Speech Set (TESS) is used to train and test both the LSTM and CNN models. This dataset consists of speech samples with labeled emotions, providing a diverse range of emotional expressions for training robust models.

